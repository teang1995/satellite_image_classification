{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNaVgZfyeXdfphaeQgOFfkN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teang1995/satellite_image_task/blob/master/ResNet_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0E8kpCBOcWp",
        "colab_type": "text"
      },
      "source": [
        "**modification history**\n",
        "\n",
        "2020.02.18\n",
        "\n",
        "- ResNet tweak-C,D 추가\n",
        "- learning rate warm-up추가\n",
        "- cosine learning rate decay 추가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ5sp0gRnc4k",
        "colab_type": "text"
      },
      "source": [
        "**import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VS8xDbZnS90",
        "colab_type": "code",
        "outputId": "499965f7-21d6-4d92-d4ba-9db05bd26363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "from torchvision import transforms as T\n",
        "import torchvision\n",
        "import cv2\n",
        "import sys\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2pMkmazhOOK",
        "colab_type": "text"
      },
      "source": [
        "**check GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdIF9CS7hNjJ",
        "colab_type": "code",
        "outputId": "3fe8cbcc-bc12-4fd8-f430-c6fdfa3bdc72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 18 08:24:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G68WKHa1hbyK",
        "colab_type": "text"
      },
      "source": [
        "**drive mount**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdivbk9XHHzs",
        "colab_type": "code",
        "outputId": "d6fa7481-15e3-4f7c-9f9d-21eea293ae6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOReAwtEn33a",
        "colab_type": "text"
      },
      "source": [
        "**Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSWm2ERqKugL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ['thermal_power_station', 'stadium', 'tennis_court', 'terrace', 'storage_tank', 'sea_ice', 'wetland', 'sparse_residential', 'ship', 'snowberg', 'rectangular_farmland', 'runway', 'parking_lot', 'overpass', 'roundabout', 'palace', 'railway_station', 'railway', 'mountain', 'river', 'ground_track_field', 'medium_residential', 'mobile_home_park', 'island', 'golf_course', 'intersection', 'lake', 'harbor', 'industrial_area', 'meadow', 'forest', 'bridge', 'church', 'freeway', 'dense_residential', 'chaparral', 'desert', 'commercial_area', 'circular_farmland', 'cloud', 'airport', 'airplane', 'beach', 'baseball_diamond', 'basketball_court']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9U_fQttn7XB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NWPU_RESISC45(Dataset):\n",
        "  '''\n",
        "  1. Satellite image dataset\n",
        "  2. 45 classes\n",
        "  '''\n",
        "  #initiallize my data , download\n",
        "  def __init__(self, root, phase = 'Train',transform = None):\n",
        "    self.phase = phase\n",
        "    imgs = pd.read_csv(root) # csv파일의 한 행\n",
        "    os.chdir(\"/content/drive/My Drive/dataset\" + \"/\" + self.phase)\n",
        "    self.img_list = [os.path.join(imgs[\"img_name\"][i]) for i in range(len(imgs))] #파일 경로로 리스트를 만듦.\n",
        "    self.label_list = [os.path.join(imgs[\"label\"][i]) for i in range(len(imgs))] #라벨 리스트를 만듦\n",
        "    #self.imgs = np.random.permutation(imgs) #dataloader의 shuffle = True로 대체\n",
        "    self.transform = transform\n",
        "  #csv파일에서 따오기로 결정\n",
        "  def __getitem__(self,index):\n",
        "    global classes\n",
        "    os.chdir(\"/content/drive/My Drive/dataset\" + \"/\" + self.phase)\n",
        "    sample = self.img_list[index]\n",
        "    img_path = sample\n",
        "    data = Image.open(img_path)\n",
        "    #data = data.convert('L') #binary chaneel\n",
        "    data = self.transform(data)\n",
        "    label = classes.index(self.label_list[index])\n",
        "    return data.float(), label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ9fwtvT0-IK",
        "colab_type": "text"
      },
      "source": [
        "**transform and Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQD7CJZbu9Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = T.Compose([\n",
        "                       T.Resize(224),\n",
        "                       #T.RandomHorizontalFlip(),                    #data\n",
        "                       #T.RandomCrop(size = [224,224],padding = 4),  #augmentation\n",
        "                       T.ToTensor(),\n",
        "                       T.Normalize(mean = [x/255 for x in [123.68,116.779,103.939]],std = [x/255 for x in [58.393,57.12,57.375]])])\n",
        "batch_size = 32\n",
        "valid_batch_size = 16\n",
        "test_batch_size = 14\n",
        "trainset = NWPU_RESISC45(root = \"/content/drive/My Drive/dataset/Train.csv\",phase = 'Train',transform = transform)\n",
        "trainloader = DataLoader(trainset,batch_size = batch_size, shuffle = True)\n",
        "\n",
        "validationset = NWPU_RESISC45(root = \"/content/drive/My Drive/dataset/Validation.csv\",phase = 'Validation',transform = transform)\n",
        "validloader = DataLoader(validationset, batch_size = valid_batch_size, shuffle = True)\n",
        "\n",
        "testset = NWPU_RESISC45(root = \"/content/drive/My Drive/dataset/Test.csv\",phase = 'Test',transform = transform)\n",
        "testloader = DataLoader(testset,batch_size = test_batch_size, shuffle = True)\n",
        "\n",
        "data_loader = {\"train\" : trainloader , \"validation\":validloader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwxIgxd8Pq2w",
        "colab_type": "text"
      },
      "source": [
        "**ResNet 18 baseline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGj0rl7NPqQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module) :\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(3,64,kernel_size = 7,stride = 2,padding = 3,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(inplace = True),\n",
        "                                nn.MaxPool2d(kernel_size = 3,stride = 2, padding = 1))\n",
        "    \n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(),\n",
        "                                nn.Conv2d(64,64,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU())\n",
        "    \n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(64,128, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU(),\n",
        "                                nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "  \n",
        "    self.layer4= nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(128,128,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU(),\n",
        "                                nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "    \n",
        "\n",
        "    self.layer5 = nn.Sequential(nn.Conv2d(128,256, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU(),\n",
        "                                nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    self.layer6 = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(256,256,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU(),\n",
        "                                nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    self.layer7 = nn.Sequential(nn.Conv2d(256,512, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    \n",
        "    self.layer8 = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(512, 1000),\n",
        "        nn.Linear(1000,45))\n",
        "    \n",
        "    #Kaiming He initialization\n",
        "    for m in self.modules():\n",
        "      if type(m) == nn.Linear or type(m) == nn.Conv2d :\n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        init.constant_(m.bias.data,0)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \n",
        "    x = self.layer1(x)\n",
        "    y = self.layer2(x)\n",
        "    x = x + y\n",
        "    y = self.layer2(x)\n",
        "    x = x + y\n",
        "    x = self.layer3(x)\n",
        "    y = self.layer4(x)\n",
        "    x = x + y\n",
        "    x = self.layer5(x)\n",
        "    y = self.layer6(x)\n",
        "    x = x + y\n",
        "    x = self.layer7(x)\n",
        "    y = self.layer8(x)\n",
        "    x = x + y\n",
        "    #print(len(x), len(x[0]),len(x[0][0]),len(x[0][0][0]))\n",
        "    x = self.gap(x)\n",
        "    #print(len(x), len(x[0]),len(x[0][0]),len(x[0][0][0]))\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.linear(x)\n",
        "    return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLZ9oIdG2Y0K",
        "colab_type": "text"
      },
      "source": [
        " **ResNet 18 with tricks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH18qYB72a9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module) :\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    #ResNet-C\n",
        "    '''\n",
        "    새롭게 추가된 layer 3,5,7_2는 ResNet-D를 구현하기 위해 추가하였음.\n",
        "    '''\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(3,64,kernel_size = 3,stride = 2,padding = 1,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(inplace = True),\n",
        "                                nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(inplace = True),\n",
        "                                nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(inplace = True),\n",
        "                                nn.MaxPool2d(kernel_size = 3,stride = 2, padding = 1))\n",
        "    \n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(),\n",
        "                                nn.Conv2d(64,64,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU())\n",
        "    \n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(64,128, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "    self.layer3_1 = nn.Sequential(nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "    self.layer3_2 = nn.Sequential(nn.AvgPool2d(kernel_size = 2,stride = 2, padding = 0),\n",
        "                                  nn.Conv2d(64,128,kernel_size = 1,bias = True))\n",
        "    \n",
        "    self.layer4= nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(128,128,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU(),\n",
        "                                nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "    \n",
        "\n",
        "    self.layer5 = nn.Sequential(nn.Conv2d(128,256, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    self.layer5_1 = nn.Sequential(nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    self.layer5_2 = nn.Sequential(nn.AvgPool2d(kernel_size = 2,stride = 2, padding = 0),\n",
        "                                  nn.Conv2d(128,256,kernel_size = 1,bias = True))\n",
        "    \n",
        "    self.layer6 = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(256,256,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU(),\n",
        "                                nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    \n",
        "    self.layer7 = nn.Sequential(nn.Conv2d(256,512, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    self.layer7_1 = nn.Sequential(nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    self.layer7_2 = nn.Sequential(nn.AvgPool2d(kernel_size = 2,stride = 2, padding = 0),\n",
        "                                  nn.Conv2d(256,512,kernel_size = 1,bias = True))\n",
        "    \n",
        "    self.layer8 = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(512, 1000),\n",
        "        nn.Linear(1000,45))\n",
        "    \n",
        "    #Kaiming He initialization\n",
        "    for m in self.modules():\n",
        "      if type(m) == nn.Linear or type(m) == nn.Conv2d :\n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        init.constant_(m.bias.data,0)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #x : 224 224 3\n",
        "    x = self.layer1(x)    #224 224 3  56 56 64\n",
        "    y = self.layer2(x)    #56 56 64   56 56 64\n",
        "    x = x + y     \n",
        "    y = self.layer2(x)    #56 56 64   56 56 64\n",
        "    x = x + y\n",
        "    y = self.layer3_2(x)  #56 56 64   28 28 128\n",
        "    x = self.layer3(x)    #56 56 64   28 28 128\n",
        "    x = x + y\n",
        "    x = self.layer3_1(x)  #28 28 128  28 28 128\n",
        "    y = self.layer4(x)    #28 28 128  28 28 128\n",
        "    x = x + y\n",
        "    y = self.layer5_2(x)  #28 28 128  14 14 256\n",
        "    x = self.layer5(x)     #28 28 128  14 14 256\n",
        "    x = x + y\n",
        "    x = self.layer5_1(x)  #14 14 256  14 14 256\n",
        "    y = self.layer6(x)    #14 14 256  14 14 256\n",
        "    x = x + y\n",
        "    y = self.layer7_2(x)  #14 14 256  7 7 512\n",
        "    x = self.layer7(x)    #14 14 256  7 7 512\n",
        "    x = x + y\n",
        "    x = self.layer7_1(x)  #7 7 512    7 7 512\n",
        "    y = self.layer8(x)    #7 7 512    7 7 512\n",
        "    x = x + y\n",
        "    #print(len(x), len(x[0]),len(x[0][0]),len(x[0][0][0]))\n",
        "    x = self.gap(x)\n",
        "    #print(len(x), len(x[0]),len(x[0][0]),len(x[0][0][0]))\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.linear(x)\n",
        "    return F.log_softmax(x)\n",
        "\n",
        "\n",
        "net = Net()\n",
        "net = net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgm6CK9spsLH",
        "colab_type": "text"
      },
      "source": [
        "**optimizer and loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W5W7QLspuGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "#optimizer = optim.SGD(net.parameters(),lr = 0.1,momentum = 0.9, weight_decay = 0.0001)\n",
        "optimizer = optim.SGD(net.parameters(),lr = 0.0,momentum = 0.9, weight_decay = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ONKWyQAvywH",
        "colab_type": "text"
      },
      "source": [
        "**Train and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhWxoeslv1GS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = './run.pth'\n",
        "tb1 = TensorBoardColab()\n",
        "\n",
        "learning_rate = 0.0\n",
        "valid_iter = 0\n",
        "train_iter = 0\n",
        "\n",
        "len_testset = len(testset)\n",
        "\n",
        "for epoch in range(105):\n",
        "  net.load_state_dict(torch.load(PATH))\n",
        "  print('Epoch {}/{}'.format(epoch + 1,105))\n",
        "  print('-' * 10)\n",
        "  for phase in [\"train\" , \"validation\" ]:\n",
        "    #running_loss는 학습 현황을 보이기 위한 loss\n",
        "    #total_loss는 한 epoch단위로 valid_loss가 증가하면 learning rate를 감소시키기 위함.\n",
        "    running_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    if phase == \"train\":\n",
        "      net.train(True)\n",
        "    else:\n",
        "      net.train(False)    \n",
        "    for i,data in enumerate(data_loader[phase]):\n",
        "      inputs,labels = data\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(inputs)\n",
        "      #아래의 내용은 train accuracy를 위함\n",
        "      loss = criterion(outputs,labels)\n",
        "      if phase == \"train\":\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      running_loss += loss.item() \n",
        "      if i % 10 == 9:   \n",
        "        print(\"[%d, %5d] %s loss : %.3f\" %(epoch + 1, i + 1, phase , running_loss / 10))\n",
        "        if phase == \"train\" :\n",
        "          tb1.save_value(\"Train Loss\", \"loss\" , train_iter, running_loss)\n",
        "          torch.save(net.state_dict(),PATH)\n",
        "          train_iter += 1\n",
        "        else : \n",
        "          tb1.save_value(\"Valid Loss\", \"loss\" , valid_iter, running_loss)\n",
        "          valid_iter += 1\n",
        "        running_loss = 0\n",
        "  #learning_rate warm up\n",
        "  if 0<= epoch <= 4 : \n",
        "    learning_rate += 0.02\n",
        "    optimizer = optim.SGD(net.parameters(),lr = learning_rate,momentum = 0.9, weight_decay = 0.0001)\n",
        "  #cosine Learning_rate decay\n",
        "  else :\n",
        "    learning_rate = 0.05 * (1 + math.cos(math.pi * ((epoch - 4 )/ 100)))\n",
        "    optimizer = optim.SGD(net.parameters(),lr = learning_rate,momentum = 0.9, weight_decay = 0.0001)\n",
        "  #test\n",
        "  correct = 0\n",
        "  for data in testloader:\n",
        "    inputs,labels = data\n",
        "    inputs,labels = Variable(inputs.cuda()),Variable(labels.cuda())\n",
        "    outputs = net(inputs)\n",
        "    pred = outputs.data.max(1,keepdim = True)[1]\n",
        "    correct += pred.eq(labels.data.view_as(pred)).sum()\n",
        "  print('Accuracy: {}/3150 ({:.0f}%)\\n'.format(correct,100. * correct / 3150))\n",
        "  tb1.save_value(\"Test Acc\" , \"acc\" , epoch, 100 * correct / len_testset)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}