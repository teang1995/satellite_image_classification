{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8KN57MGaHxPGvlbhdscFp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teang1995/satellite_image_classification/blob/master/ResNet_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ5sp0gRnc4k",
        "colab_type": "text"
      },
      "source": [
        "**import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VS8xDbZnS90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "from torchvision import transforms as T\n",
        "import torchvision\n",
        "import cv2\n",
        "import sys\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpDC-gb33LPH",
        "colab_type": "text"
      },
      "source": [
        "**check cuda**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mzmaapt2qUo",
        "colab_type": "code",
        "outputId": "e82bb03f-f4bd-4b19-baf9-07116ef6dfed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G68WKHa1hbyK",
        "colab_type": "text"
      },
      "source": [
        "**drive mount**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdivbk9XHHzs",
        "colab_type": "code",
        "outputId": "58aca8d5-bf90-4982-f77d-fb5c57d8a58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxWe1n5NEYCp",
        "colab_type": "code",
        "outputId": "31301fd1-f7c6-415c-bc59-e74b77a1a292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "writer = SummaryWriter(\"runs/simple_classification\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSWm2ERqKugL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ['thermal_power_station', 'stadium', 'tennis_court', 'terrace', 'storage_tank', 'sea_ice', 'wetland', 'sparse_residential', 'ship', 'snowberg', 'rectangular_farmland', 'runway', 'parking_lot', 'overpass', 'roundabout', 'palace', 'railway_station', 'railway', 'mountain', 'river', 'ground_track_field', 'medium_residential', 'mobile_home_park', 'island', 'golf_course', 'intersection', 'lake', 'harbor', 'industrial_area', 'meadow', 'forest', 'bridge', 'church', 'freeway', 'dense_residential', 'chaparral', 'desert', 'commercial_area', 'circular_farmland', 'cloud', 'airport', 'airplane', 'beach', 'baseball_diamond', 'basketball_court']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOReAwtEn33a",
        "colab_type": "text"
      },
      "source": [
        "**Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9U_fQttn7XB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NWPU_RESISC45(Dataset):\n",
        "  '''\n",
        "  1. Satellite image dataset\n",
        "  2. 45 classes\n",
        "  '''\n",
        "  #initiallize my data , download\n",
        "  def __init__(self, root, phase = 'Train',transform = None):\n",
        "    self.phase = phase\n",
        "    imgs = pd.read_csv(root) # csv파일의 한 행\n",
        "    os.chdir(\"/content/drive/My Drive/dataset\" + \"/\" + self.phase)\n",
        "    self.img_list = [os.path.join(imgs[\"img_name\"][i]) for i in range(len(imgs))] #파일 경로로 리스트를 만듦.\n",
        "    self.label_list = [os.path.join(imgs[\"label\"][i]) for i in range(len(imgs))] #라벨 리스트를 만듦\n",
        "    #self.imgs = np.random.permutation(imgs) #dataloader의 shuffle = True로 대체\n",
        "    self.transform = transform\n",
        "  #csv파일에서 따오기로 결정\n",
        "  def __getitem__(self,index):\n",
        "    global classes\n",
        "    os.chdir(\"/content/drive/My Drive/dataset\" + \"/\" + self.phase)\n",
        "    sample = self.img_list[index]\n",
        "    img_path = sample\n",
        "    data = Image.open(img_path)\n",
        "    #data = data.convert('L') #binary chaneel\n",
        "    data = self.transform(data)\n",
        "    label = classes.index(self.label_list[index])\n",
        "    return data.float(), label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ9fwtvT0-IK",
        "colab_type": "text"
      },
      "source": [
        "**transform and Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQD7CJZbu9Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = T.Compose([\n",
        "                       T.Resize(224),\n",
        "                       T.RandomHorizontalFlip(),                    #data\n",
        "                       #T.RandomCrop(size = [224,224],padding = 4),  #augmentation\n",
        "                       T.ToTensor()])\n",
        "batch_size = 32\n",
        "valid_batch_size = 8\n",
        "test_batch_size = 14\n",
        "trainset = NWPU_RESISC45(root = \"/content/drive/My Drive/dataset/Train.csv\",phase = 'Train',transform = transform)\n",
        "trainloader = DataLoader(trainset,batch_size = batch_size, shuffle = True)\n",
        "\n",
        "validationset = NWPU_RESISC45(root = \"/content/drive/My Drive/dataset/Validation.csv\",phase = 'Validation',transform = transform)\n",
        "validloader = DataLoader(validationset, batch_size = valid_batch_size, shuffle = True)\n",
        "\n",
        "testset = NWPU_RESISC45(root = \"/content/drive/My Drive/dataset/Test.csv\",phase = 'Test',transform = transform)\n",
        "testloader = DataLoader(testset,batch_size = test_batch_size, shuffle = True)\n",
        "\n",
        "data_loader = {\"train\" : trainloader , \"validation\":validloader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLZ9oIdG2Y0K",
        "colab_type": "text"
      },
      "source": [
        "**VGGNET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH18qYB72a9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module) :\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(3,64,kernel_size = 7,stride = 2,padding = 3,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(inplace = True),\n",
        "                                nn.MaxPool2d(kernel_size = 3,stride = 2, padding = 1))\n",
        "    \n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1,bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU(),\n",
        "                                nn.Conv2d(64,64,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(64),nn.ReLU())\n",
        "    \n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(64,128, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU(),\n",
        "                                nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "  \n",
        "    self.layer4= nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(128,128,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU(),\n",
        "                                nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(128),nn.ReLU())\n",
        "    \n",
        "\n",
        "    self.layer5 = nn.Sequential(nn.Conv2d(128,256, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU(),\n",
        "                                nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    self.layer6 = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(256,256,kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU(),\n",
        "                                nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(256),nn.ReLU())\n",
        "    self.layer7 = nn.Sequential(nn.Conv2d(256,512, kernel_size = 3, stride = 2, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    \n",
        "    self.layer8 = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU(),\n",
        "                                nn.Conv2d(512,512, kernel_size = 3, stride = 1, padding = 1, bias = True),\n",
        "                                nn.BatchNorm2d(512),nn.ReLU())\n",
        "    self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(512, 1000),\n",
        "        nn.Linear(1000,45))\n",
        "    \n",
        "    #Kaiming He initialization\n",
        "    for m in self.modules():\n",
        "      if type(m) == nn.Linear or type(m) == nn.Conv2d :\n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        init.constant_(m.bias.data,0)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \n",
        "    x = self.layer1(x)\n",
        "    y = self.layer2(x)\n",
        "    x = x + y\n",
        "    y = self.layer2(x)\n",
        "    x = x + y\n",
        "    x = self.layer3(x)\n",
        "    y = self.layer4(x)\n",
        "    x = x + y\n",
        "    x = self.layer5(x)\n",
        "    y = self.layer6(x)\n",
        "    x = x + y\n",
        "    x = self.layer7(x)\n",
        "    y = self.layer8(x)\n",
        "    x = x + y\n",
        "    #print(len(x), len(x[0]),len(x[0][0]),len(x[0][0][0]))\n",
        "    x = self.gap(x)\n",
        "    #print(len(x), len(x[0]),len(x[0][0]),len(x[0][0][0]))\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.linear(x)\n",
        "    return F.log_softmax(x)\n",
        "#cuda사용 가능하면 gpu사용, 아니면 cpu로 연산\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = Net()\n",
        "net = net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgm6CK9spsLH",
        "colab_type": "text"
      },
      "source": [
        "**optimizer and loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W5W7QLspuGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.SGD(net.parameters(),lr = 0.1,momentum = 0.9, weight_decay = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ONKWyQAvywH",
        "colab_type": "text"
      },
      "source": [
        "**Train and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhWxoeslv1GS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = 999999.0\n",
        "learning_rate = 0.1\n",
        "PATH = \"/content/drive/My Drive/weight.txt\"\n",
        "torch.save(net.state_dict(), PATH)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "print(\"빰\")\n",
        "for epoch in range(50):\n",
        "  print('Epoch {}/{}'.format(epoch + 1,50))\n",
        "  print('-' * 10)\n",
        "  for phase in [\"train\" , \"validation\" ]:\n",
        "    #running_loss는 학습 현황을 보이기 위한 loss\n",
        "    #total_loss는 한 epoch단위로 valid_loss가 증가하면 learning rate를 감소시키기 위함.\n",
        "    running_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    if phase == \"train\":\n",
        "      net.train(True)\n",
        "    else:\n",
        "      net.train(False)    \n",
        "    for i,data in enumerate(data_loader[phase]):\n",
        "      inputs,labels = data\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(inputs)\n",
        "      #아래의 내용은 train accuracy를 위함\n",
        "      loss = criterion(outputs,labels)\n",
        "      if phase == \"train\":\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() \n",
        "      if phase == \"validation\":         #validation 단계에서\n",
        "        total_loss += loss.item()       #total_loss 갱신\n",
        "      if i % 10 == 9:   \n",
        "        print(\"[%d, %5d] %s loss : %.3f\" %(epoch + 1, i + 1, phase , running_loss / 10))\n",
        "        writer.add_scalar(phase + \"loss\", running_loss/10, epoch * len(data_loader[phase]) + i)\n",
        "        running_loss = 0.0\n",
        "  \n",
        "  #save model weights and bias\n",
        "  torch.save(model.state_dict(), PATH)\n",
        "  #test\n",
        "  correct = 0\n",
        "  for data in testloader:\n",
        "    inputs,labels = data\n",
        "    inputs,labels = Variable(inputs.cuda()),Variable(labels.cuda())\n",
        "    outputs = net(inputs)\n",
        "    pred = outputs.data.max(1,keepdim = True)[1]\n",
        "    correct += pred.eq(labels.data.view_as(pred)).sum()\n",
        "  print('Accuracy: {}/3150 ({:.0f}%)\\n'.format(correct,100. * correct / 3150))\n",
        "\n",
        "  #validation loss가 커지면\n",
        "  if True :\n",
        "    total_loss = 999999.0\n",
        "    #optimizer의 learning rate를 10으로 나눈다.\n",
        "    learning_rate = learning_rate / 10\n",
        "    optimizer = optim.SGD(net.parameters(),lr = lr,momentum = 0.9, weight_decay = 0.0001)\n",
        "    #weight값을 이전의 것으로 갱신한다.\n",
        "    net.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}